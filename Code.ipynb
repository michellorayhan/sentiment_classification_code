{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import ast\n",
    "import joblib\n",
    "import warnings\n",
    "import nltk\n",
    "from wordcloud import WordCloud\n",
    "from googletrans import Translator\n",
    "from langdetect import detect\n",
    "from collections import Counter\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Text Processing\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Set warnings and plotting\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATASET INITIALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('rm_dataset.csv')\n",
    "df = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.notnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rating'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manual Feature Selection\n",
    "df = df.drop('name', axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom Binning\n",
    "def custom_binning(rating):\n",
    "    if rating < 2.9:\n",
    "        return 'bad'\n",
    "    elif 2.9 <= rating < 3.9:\n",
    "        return 'neutral'\n",
    "    elif 3.9 <= rating <= 5.0:\n",
    "        return 'positive'\n",
    "\n",
    "df['category'] = df['rating'].apply(custom_binning)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Cleaning\n",
    "def process_text(text):\n",
    "    text = text.lower() #Menghubah huruf ke huruf kecil\n",
    "    text = re.sub('@[^\\s]+', '', text) #Menghapus @\n",
    "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', ' ', text) #Menghapus URL\n",
    "    text = re.sub(r\"\\d+\", \" \", str(text)) #Menghapus angka\n",
    "    text = re.sub('&quot;', \" \", text) #Menghapus entity HTML\n",
    "    text = re.sub(r\"\\b[a-zA-Z]\\b\", \"\", str(text)) #Menghapus karakter tunggal\n",
    "    text = re.sub(r\"[^\\w\\s]\", \" \", str(text)) #Menghapus tanda baca\n",
    "    text = re.sub(r'(.)\\1+', r'\\1\\1', text) #Menghapus karakter yang diulang\n",
    "    text = re.sub(r\"\\s+\", \" \", str(text)) #Menghapus spasi berlebih\n",
    "    return text\n",
    "\n",
    "df['desc'] = df['desc'].apply(process_text)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization\n",
    "def tokenize_text(kalimat):\n",
    "    # Tokenizes the input sentence into words\n",
    "    tokens = nltk.tokenize.word_tokenize(kalimat)\n",
    "    return tokens\n",
    "\n",
    "df['token'] = df['desc'].apply(tokenize_text)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Token Translation\n",
    "translator = Translator()\n",
    "def translate_tokens(tokens):\n",
    "    translated_tokens = []\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            lang = detect(token)\n",
    "            if lang != 'id':  # Jika bukan bahasa Indonesia\n",
    "                translated_token = translator.translate(token, src=lang, dest='id').text\n",
    "            else:\n",
    "                translated_token = token\n",
    "        except:\n",
    "            translated_token = token  # Jika deteksi bahasa gagal, tetap menggunakan token asli\n",
    "        translated_tokens.append(translated_token)\n",
    "    return translated_tokens\n",
    "\n",
    "df['translated_token'] = df['token'].apply(translate_tokens)\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('DM_2.csv')\n",
    "df = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_tokenize_text(text):\n",
    "    # Data Cleaning\n",
    "    text = text.lower()  # Mengubah huruf ke huruf kecil\n",
    "    text = re.sub('@[^\\s]+', '', text)  # Menghapus @\n",
    "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', ' ', text)  # Menghapus URL\n",
    "    text = re.sub(r\"\\d+\", \" \", str(text))  # Menghapus angka\n",
    "    text = re.sub('&quot;', \" \", text)  # Menghapus entity HTML\n",
    "    text = re.sub(r\"\\b[a-zA-Z]\\b\", \"\", str(text))  # Menghapus karakter tunggal\n",
    "    text = re.sub(r\"[^\\w\\s]\", \" \", str(text))  # Menghapus tanda baca\n",
    "    text = re.sub(r'(.)\\1+', r'\\1\\1', text)  # Menghapus karakter yang diulang\n",
    "    text = re.sub(r\"\\s+\", \" \", str(text))  # Menghapus spasi berlebih\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = nltk.tokenize.word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "df['translated_token'] = df['translated_token'].apply(process_and_tokenize_text)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stopword Removal\n",
    "factory = StopWordRemoverFactory()\n",
    "\n",
    "additional = ['yg','mo', 'woi', 'nih', 'sih']\n",
    "\n",
    "stopwords = factory.get_stop_words()\n",
    "stopwords = stopwords + additional\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopword_removal(tokens):\n",
    "    cleaned_tokens = []\n",
    "    for token in tokens:\n",
    "        if token not in stopwords:\n",
    "            cleaned_tokens.append(token)\n",
    "    return cleaned_tokens\n",
    "\n",
    "df['stop'] = df['translated_token'].apply(stopword_removal)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "stem_factory = StemmerFactory()\n",
    "stemmer = stem_factory.create_stemmer()\n",
    "\n",
    "def stemming_text(tokens):\n",
    "    hasil = [stemmer.stem(token) for token in tokens]\n",
    "    return hasil\n",
    "\n",
    "df['stem'] = df['stop'].apply(stemming_text)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove duplicate tokens\n",
    "def remove_duplicates(tokens):\n",
    "    return list(dict.fromkeys(tokens))\n",
    "\n",
    "df['clean_token'] = df['stem'].apply(remove_duplicates)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mengubah token jadi teks\n",
    "def convert(token_str):\n",
    "    try:\n",
    "        if isinstance(token_str, str):            \n",
    "            clean_str = token_str.replace(\"'\", '\"')\n",
    "            token_list = ast.literal_eval(clean_str)\n",
    "            return ' '.join(token_list)\n",
    "        else:            \n",
    "            return ' '.join(token_str)\n",
    "    except Exception as e:        \n",
    "        print(f\"Error converting: {token_str}\\nException: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "df['clean_text'] = df['clean_token'].apply(convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(kernel='linear', C=1, random_state=42)\n",
    "logistic = LogisticRegression(random_state=42, max_iter=1000)\n",
    "naive_bayes = MultinomialNB()\n",
    "def evaluate_model(clf, X, y, splits, le):\n",
    "    # Initialize dictionaries to store metrics for each label\n",
    "    label_metrics = {label: {'precision': [], 'recall': [], 'f1_score': []} for label in le.classes_}\n",
    "    accuracies = []\n",
    "    weighted_precisions = []\n",
    "    weighted_recalls = []\n",
    "    weighted_f1_scores = []\n",
    "\n",
    "    for train_index, test_index in splits:\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Train the model\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # Predict on the test set\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "        # Get weighted precision, recall, and F1 score\n",
    "        weighted_precision, weighted_recall, weighted_f1, _ = precision_recall_fscore_support(\n",
    "            y_test, y_pred, average='weighted'\n",
    "        )\n",
    "        \n",
    "        # Store weighted metrics\n",
    "        weighted_precisions.append(weighted_precision)\n",
    "        weighted_recalls.append(weighted_recall)\n",
    "        weighted_f1_scores.append(weighted_f1)\n",
    "\n",
    "        # Get precision, recall, and F1 score for each label (without specifying 'average')\n",
    "        per_label_precision, per_label_recall, per_label_f1, _ = precision_recall_fscore_support(\n",
    "            y_test, y_pred, labels=range(len(le.classes_)), average=None\n",
    "        )\n",
    "\n",
    "        # Store metrics for each label\n",
    "        for i, label in enumerate(le.classes_):\n",
    "            label_metrics[label]['precision'].append(per_label_precision[i])\n",
    "            label_metrics[label]['recall'].append(per_label_recall[i])\n",
    "            label_metrics[label]['f1_score'].append(per_label_f1[i])\n",
    "\n",
    "        # Print classification report for each fold\n",
    "        # report = classification_report(y_test, y_pred, target_names=le.classes_)\n",
    "        # print(f\"Fold report:\\n{report}\")\n",
    "\n",
    "    # Calculate and print the average for each label\n",
    "    for label in le.classes_:\n",
    "        avg_precision = np.mean(label_metrics[label]['precision'])\n",
    "        avg_recall = np.mean(label_metrics[label]['recall'])\n",
    "        avg_f1 = np.mean(label_metrics[label]['f1_score'])\n",
    "\n",
    "        print(f\"{label} - Average Precision: {avg_precision:.3f}\")\n",
    "        print(f\"{label} - Average Recall: {avg_recall:.3f}\")\n",
    "        print(f\"{label} - Average F1-Score: {avg_f1:.3f}\")\n",
    "        print()  # Blank line for readability\n",
    "\n",
    "    # Calculate and display the overall weighted precision, recall, F1-score, and accuracy\n",
    "    overall_weighted_precision = np.mean(weighted_precisions)\n",
    "    overall_weighted_recall = np.mean(weighted_recalls)\n",
    "    overall_weighted_f1_score = np.mean(weighted_f1_scores)\n",
    "    overall_average_accuracy = np.mean(accuracies)\n",
    "\n",
    "    print(f\"Overall Weighted Precision: {overall_weighted_precision:.3f}\")\n",
    "    print(f\"Overall Weighted Recall: {overall_weighted_recall:.3f}\")\n",
    "    print(f\"Overall Weighted F1-Score: {overall_weighted_f1_score:.3f}\")\n",
    "    print(f\"Overall Average Accuracy: {overall_average_accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target variable\n",
    "X = df['clean_text']\n",
    "y = df['category']\n",
    "\n",
    "# Convert text data to TF-IDF features\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_tfidf = vectorizer.fit_transform(X)\n",
    "\n",
    "# Encode labels for stratification\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# Apply Stratified K-Fold cross-validation\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "splits = list(skf.split(X_tfidf, y_encoded))\n",
    "    \n",
    "# Apply RandomOverSampler to balance the classes\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_ros, y_ros = ros.fit_resample(X_tfidf, y)\n",
    "\n",
    "# Encode labels for stratification\n",
    "y_encoded_ros = le.fit_transform(y_ros)\n",
    "\n",
    "# Apply Stratified K-Fold cross-validation with RandomOverSampler\n",
    "splits_ros = list(skf.split(X_ros, y_encoded_ros))\n",
    "\n",
    "# Evaluate models with RandomOverSampler\n",
    "print(\"\\nEvaluating models with TF-IDF...\")\n",
    "\n",
    "# Evaluate SVM\n",
    "print(\"Evaluating SVM...\\n\")\n",
    "evaluate_model(svm, X_ros, y_encoded_ros, splits_ros, le)\n",
    "\n",
    "# Evaluate Logistic Regression\n",
    "print(\"\\nEvaluating Logistic Regression...\\n\")\n",
    "evaluate_model(logistic, X_ros, y_encoded_ros, splits_ros, le)\n",
    "\n",
    "# Evaluate Naive Bayes\n",
    "print(\"\\nEvaluating Naive Bayes...\\n\")\n",
    "evaluate_model(naive_bayes, X_ros, y_encoded_ros, splits_ros, le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menghitung frekuensi kemunculan tiap kategori\n",
    "y_encoded_distribution = np.bincount(y_encoded)\n",
    "y_encoded_distribution_df = pd.DataFrame({'Category': le.classes_, 'Frequency': y_encoded_distribution})\n",
    "\n",
    "# Menampilkan persebaran dalam bentuk tabel\n",
    "print(\"Persebaran y_encoded:\")\n",
    "print(y_encoded_distribution_df)\n",
    "\n",
    "# Membuat grafik bar\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(y_encoded_distribution_df['Category'], y_encoded_distribution_df['Frequency'], color='skyblue')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Categories in y_encoded')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menghitung frekuensi kemunculan tiap kategori setelah RandomOverSampler\n",
    "y_encoded_ros_distribution = np.bincount(y_encoded_ros)\n",
    "y_encoded_ros_distribution_df = pd.DataFrame({'Category': le.classes_, 'Frequency': y_encoded_ros_distribution})\n",
    "\n",
    "# Menampilkan persebaran dalam bentuk tabel\n",
    "print(\"\\nPersebaran y_encoded_ros:\")\n",
    "print(y_encoded_ros_distribution_df)\n",
    "\n",
    "# Membuat grafik bar\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(y_encoded_ros_distribution_df['Category'], y_encoded_ros_distribution_df['Frequency'], color='lightgreen')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Categories in y_encoded_ros (After RandomOverSampler)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target variable\n",
    "X = df['clean_text']\n",
    "y = df['category']\n",
    "\n",
    "# Convert text data to BoW features\n",
    "vectorizer = CountVectorizer()\n",
    "X_bow = vectorizer.fit_transform(X)\n",
    "\n",
    "# Encode labels for stratification\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# Apply Stratified K-Fold cross-validation\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "splits = list(skf.split(X_bow, y_encoded))\n",
    "\n",
    "# Apply RandomOverSampler to balance the classes\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_ros, y_ros = ros.fit_resample(X_bow, y)\n",
    "\n",
    "# Encode labels for stratification\n",
    "y_encoded_ros = le.fit_transform(y_ros)\n",
    "\n",
    "# Apply Stratified K-Fold cross-validation with RandomOverSampler\n",
    "splits_ros = list(skf.split(X_ros, y_encoded_ros))\n",
    "\n",
    "# Evaluate models with RandomOverSampler\n",
    "print(\"\\nEvaluating models with BoW...\")\n",
    "\n",
    "# Evaluate SVM\n",
    "print(\"Evaluating SVM...\\n\")\n",
    "evaluate_model(svm, X_ros, y_encoded_ros, splits_ros, le)\n",
    "\n",
    "# Evaluate Logistic Regression\n",
    "print(\"\\nEvaluating Logistic Regression...\\n\")\n",
    "evaluate_model(logistic, X_ros, y_encoded_ros, splits_ros, le)\n",
    "\n",
    "# Evaluate Naive Bayes\n",
    "print(\"\\nEvaluating Naive Bayes...\\n\")\n",
    "evaluate_model(naive_bayes, X_ros, y_encoded_ros, splits_ros, le)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
